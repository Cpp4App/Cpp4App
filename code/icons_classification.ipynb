{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFbck5eAjuFC"
   },
   "source": [
    "# This is the notebook for project Contextual Privacy Policy for Mobile Apps. Ultimately, we aim to achieve the privacy-related icons detection and classification.\n",
    "\n",
    "## We first to adapt an existing icons classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glU9wUUaQvK7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLED_72fRq3S",
    "outputId": "9a938756-814f-4a29-cf7f-d36a378d5204"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Kt8IkKvVfHP",
    "outputId": "eaba2dd4-e096-4e22-b4a3-d19718d664f4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9c3u-rAywGw",
    "outputId": "6cf92722-e5ef-445f-8890-7fda843b833c"
   },
   "outputs": [],
   "source": [
    "# check CPU\n",
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEZpduJGzWoC",
    "outputId": "71b506e2-a0f6-45c8-85a8-3792a4613007"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5OsvgzoQvLF",
    "outputId": "6803a0b2-1ee5-4a17-d4fa-218acd167f44"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "x_train = np.load(\"/content/gdrive/MyDrive/mobile-semantics-classification/training_x.npy\")\n",
    "y_train = np.load(\"/content/gdrive/MyDrive/mobile-semantics-classification/training_y.npy\")\n",
    "x_test = np.load(\"/content/gdrive/MyDrive/mobile-semantics-classification/validation_x.npy\")\n",
    "y_test = np.load(\"/content/gdrive/MyDrive/mobile-semantics-classification/validation_y.npy\")\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# num_classes = np.unique(y_train).shape[0]\n",
    "# Print Unique Icon Classes, 99 classes\n",
    "# print(np.unique(y_train))\n",
    "# print(num_classes, ' classes')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#print(y_train[0])\n",
    "# y_train = torch.tensor(y_train)\n",
    "# y_train = F.one_hot(y_train.to(torch.int64), num_classes=num_classes)\n",
    "# y_train = np.array(y_train)\n",
    "#print(y_train[0])\n",
    "\n",
    "# idx_train = np.where((y_train == [72]) | (y_train == [42])\n",
    "#  | (y_train == [91]) | (y_train == [6]) | (y_train == [40])\n",
    "#   | (y_train == [43]) | (y_train == [82]) | (y_train == [3]) | (y_train == [68])\n",
    "#    )\n",
    "\n",
    "# x_train = x_train[idx_train[0]]\n",
    "# y_train = y_train[idx_train[0]]\n",
    "\n",
    "# print(np.where(y_train == [6]))\n",
    "\n",
    "# change the selected labels to 0-11\n",
    "# y_train[np.where(y_train==[3])] = [7]\n",
    "# y_train[np.where(y_train==[72])] = [0]\n",
    "# y_train[np.where(y_train==[42])] = [1]\n",
    "\n",
    "# y_train[np.where(y_train==[91])] = [2]\n",
    "# y_train[np.where(y_train==[6])] = [3]\n",
    "\n",
    "# y_train[np.where(y_train==[40])] = [4]\n",
    "# y_train[np.where(y_train==[43])] = [5]\n",
    "# y_train[np.where(y_train==[82])] = [6]\n",
    "# y_train[np.where(y_train==[68])] = [8]\n",
    "\n",
    "\n",
    "# print(\"idx_train length: \", idx_train[0].shape)\n",
    "# print('x_train_selected shape:', x_train.shape)\n",
    "# print('y_train_selected shape:', y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# idx_test = np.where((y_test == [72]) | (y_test == [42])\n",
    "#  | (y_test == [91]) | (y_test == [6]) | (y_test == [40])\n",
    "#   | (y_test == [43]) | (y_test == [82]) | (y_test == [3]) | (y_test == [68])\n",
    "#    )\n",
    "\n",
    "# x_test = x_test[idx_test[0]]\n",
    "# y_test = y_test[idx_test[0]]\n",
    "\n",
    "# change the selected labels to 0-11\n",
    "# y_test[np.where(y_test==[3])] = [7]\n",
    "# y_test[np.where(y_test==[72])] = [0]\n",
    "# y_test[np.where(y_test==[42])] = [1]\n",
    "\n",
    "# y_test[np.where(y_test==[91])] = [2]\n",
    "# y_test[np.where(y_test==[6])] = [3]\n",
    "\n",
    "# y_test[np.where(y_test==[40])] = [4]\n",
    "# y_test[np.where(y_test==[43])] = [5]\n",
    "# y_test[np.where(y_test==[82])] = [6]\n",
    "# y_test[np.where(y_test==[68])] = [8]\n",
    "\n",
    "\n",
    "# print(\"idx_test length: \", idx_test[0].shape)\n",
    "# print('x_test_selected shape:', x_test.shape)\n",
    "# print('y_test_selected shape:', y_test.shape)\n",
    "\n",
    "print(\"reshape: \")\n",
    "x_train = x_train.reshape([x_train.shape[0], x_train.shape[3], x_train.shape[1], x_train.shape[2]])\n",
    "x_test = x_test.reshape([x_test.shape[0], x_test.shape[3], x_test.shape[1], x_test.shape[2]])\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "print(num_classes, ' classes')\n",
    "# print(np.unique(y_train))\n",
    "\n",
    "# num_classes = np.unique(y_test).shape[0]\n",
    "# print(num_classes, ' classes')\n",
    "# print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ddBVOF7QvLH",
    "outputId": "ff10d9ee-34ad-4c60-b9f3-4deadfc31400"
   },
   "outputs": [],
   "source": [
    "# use dataloader\n",
    "\n",
    "# print(x_train[0])\n",
    "# print(x_train.mean())\n",
    "\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "\n",
    "# mean = mean/len(x_train)\n",
    "# std = std/len(x_train)\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"std: \", std)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std), # featurewise centering\n",
    "\n",
    "    # data augmentation\n",
    "    # transforms.RandomAffine(degrees=0, translate=(0.1,0.1)), # randomly moved along the x- and y-axis by up to 10%\n",
    "\n",
    "    # add more operations\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(15)\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std), # featurewise centering\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image = self.images[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "\n",
    "            image = self.transform(image)\n",
    "            image = image.permute(1,0,2)\n",
    "\n",
    "\n",
    "        return image, label\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(CustomDataset(x_train[:113840],y_train[:113840],transform_train), batch_size=batch_size, shuffle=True)\n",
    "# validation_loader = DataLoader(CustomDataset(x_train[9600:11200],y_train[9600:11200]), batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(CustomDataset(x_test[:1600],y_test[:1600]), batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(CustomDataset(x_test[0:4800],y_test[0:4800],transform_val), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ualaqLlPQvLI"
   },
   "outputs": [],
   "source": [
    "# choose model\n",
    "\n",
    "# -------------- use our own model -------------\n",
    "# model = Net().to(device)\n",
    "\n",
    "# --------------- use resnet18 -------------------\n",
    "\n",
    "# from torchvision import models\n",
    "\n",
    "# model = models.resnet18().to(device)\n",
    "# in_feature_num = model.fc.in_features\n",
    "# model.fc = nn.Linear(in_feature_num, 99)\n",
    "# # model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3), padding=(3,3), stride=(2,2), bias=False)\n",
    "# model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(5,5), padding=(3,3), stride=(2,2), bias=False)\n",
    "\n",
    "# --------------- use resnet50 -------------------\n",
    "\n",
    "# from torchvision import models\n",
    "\n",
    "# model = models.resnet50().to(device)\n",
    "# in_feature_num = model.fc.in_features\n",
    "# model.fc = nn.Linear(in_feature_num, 99)\n",
    "# # model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3), padding=(3,3), stride=(2,2), bias=False)\n",
    "# model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(5,5), padding=(3,3), stride=(2,2), bias=False)\n",
    "\n",
    "# -------------- mobilenet -----------------------\n",
    "\n",
    "# from torchvision import models\n",
    "\n",
    "# # model = models.mobilenet_v2().to(device)\n",
    "# model = models.mobilenet_v3_small().to(device)\n",
    "\n",
    "# model.features[0][0] = torch.nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "# num_features = model.classifier[1].in_features\n",
    "# model.classifier[1] = torch.nn.Linear(num_features, 99)\n",
    "\n",
    "# -------------- mobilenet_v3 -----------------------\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "# model = models.mobilenet_v2().to(device)\n",
    "model = models.mobilenet_v3_small().to(device)\n",
    "\n",
    "model.features[0][0] = torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "model.classifier[-1] = nn.Linear(1024, 99)\n",
    "\n",
    "# --------------- end ----------------------------\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o71mNdlzQvLI",
    "outputId": "d1b6f8c1-8ef8-40b9-cd22-974db5a123bf"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# Put all hyperparameters together\n",
    "initial_leanring_rate = 0.001\n",
    "learning_rate_weight_decay = 3e-4\n",
    "epoch_num = 20\n",
    "\n",
    "\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.RMSprop(params=model.parameters(), lr=initial_leanring_rate, weight_decay=learning_rate_weight_decay)\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=initial_leanring_rate, weight_decay=learning_rate_weight_decay)\n",
    "\n",
    "best_model = model\n",
    "\n",
    "best_accuracy = 0\n",
    "\n",
    "best_epoch = 0\n",
    "\n",
    "train_accuracy_all = []\n",
    "\n",
    "validation_accuracy_all = []\n",
    "\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_accuracy = 0\n",
    "    validation_accuracy = 0\n",
    "    train_loss = 0\n",
    "    validation_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_output = model(data[0].to(device))\n",
    "\n",
    "        data[1] = data[1].view(data[1].shape[0])\n",
    "\n",
    "        batch_loss = loss(train_output, data[1].to(device))\n",
    "        # print(train_output.size())\n",
    "        # print(data[1].size())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_accuracy = train_accuracy + np.sum(np.argmax(train_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy())\n",
    "        # print('correct train in this batch: ', np.sum(np.argmax(train_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy()))\n",
    "        # print('all correct train: ', train_accuracy)\n",
    "        train_loss = train_loss + batch_loss.item()\n",
    "\n",
    "        # print(\"training | num of right labelling in this batch: \", np.sum(np.argmax(train_output.data.numpy(),axis=1) == data[1].numpy()))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validation_loader):\n",
    "\n",
    "            validation_output = model(data[0].to(device))\n",
    "\n",
    "            data[1] = data[1].view(data[1].shape[0])\n",
    "\n",
    "            batch_loss = loss(validation_output, data[1].to(device))\n",
    "\n",
    "            validation_accuracy = validation_accuracy + np.sum(np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy())\n",
    "            # print('correct val in this batch: ', np.sum(np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy()))\n",
    "            # print('all correct val: ', validation_accuracy)\n",
    "            validation_loss = validation_loss + batch_loss.item()\n",
    "\n",
    "            # print(\"validation | num of right labelling: \", np.sum(np.argmax(validation_output.data.numpy(),axis=1) == data[1].numpy()))\n",
    "\n",
    "            c = (np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy())\n",
    "            c = c.squeeze()\n",
    "            for i in range(batch_size):\n",
    "              i_label = data[1][i]\n",
    "              class_correct[i_label] += c[i]\n",
    "              class_total[i_label] += 1\n",
    "\n",
    "        print(\"epoch: \", epoch + 1)\n",
    "        print(\"train accuracy: \", train_accuracy/(train_loader.__len__()*batch_size))\n",
    "        print(\"train loss: \", train_loss/(train_loader.__len__()*batch_size))\n",
    "        print(\"validation accuracy: \", validation_accuracy/(validation_loader.__len__()*batch_size))\n",
    "        print(\"validation loss: \", validation_loss/(validation_loader.__len__()*batch_size))\n",
    "\n",
    "        print(\"time cost: %2.2f s\" % (time.time() - start_time))\n",
    "\n",
    "        num_90_percent_correct = 0\n",
    "        num_80_percent_correct = 0\n",
    "        num_70_percent_correct = 0\n",
    "        num_60_percent_correct = 0\n",
    "        num_50_percent_correct = 0\n",
    "        num_under_50_percent_correct = 0\n",
    "\n",
    "        for i in range(0, num_classes):\n",
    "\n",
    "          class_percent_correct = np.round(100 * class_correct[i] / class_total[i], 2)\n",
    "\n",
    "          print(f\"The accuracy of class {i}: {class_percent_correct}%\")\n",
    "\n",
    "          if class_percent_correct >= 90:\n",
    "            num_90_percent_correct = num_90_percent_correct + 1\n",
    "          elif class_percent_correct >= 80:\n",
    "            num_80_percent_correct = num_80_percent_correct + 1\n",
    "          elif class_percent_correct >= 70:\n",
    "            num_70_percent_correct = num_70_percent_correct + 1\n",
    "          elif class_percent_correct >= 60:\n",
    "            num_60_percent_correct = num_60_percent_correct + 1\n",
    "          elif class_percent_correct >= 50:\n",
    "            num_50_percent_correct = num_50_percent_correct + 1\n",
    "          else:\n",
    "            num_under_50_percent_correct = num_under_50_percent_correct + 1\n",
    "\n",
    "        print(\"The number of classes with accuracy equal to or more than 90%: \", num_90_percent_correct)\n",
    "        print(\"The number of classes with accuracy equal to or more than 80%: \", num_80_percent_correct)\n",
    "        print(\"The number of classes with accuracy equal to or more than 70%: \", num_70_percent_correct)\n",
    "        print(\"The number of classes with accuracy equal to or more than 60%: \", num_60_percent_correct)\n",
    "        print(\"The number of classes with accuracy equal to or more than 50%: \", num_50_percent_correct)\n",
    "        print(\"The number of classes with accuracy lower than 50%: \", num_under_50_percent_correct)\n",
    "\n",
    "        train_accuracy_all.append(train_accuracy/(train_loader.__len__()*batch_size))\n",
    "        validation_accuracy_all.append(validation_accuracy/(validation_loader.__len__()*batch_size))\n",
    "\n",
    "\n",
    "\n",
    "#         print(\"epoch training | num of right labelling: \", train_accuracy)\n",
    "#         print(\"epoch training | len of train_loader: \", train_loader.__len__()*batch_size)\n",
    "#         print(\"epoch val | num of right labelling: \", validation_accuracy)\n",
    "#         print(\"epoch val | len of validation_loader: \", validation_loader.__len__()*batch_size)\n",
    "\n",
    "        if best_accuracy < (validation_accuracy/validation_loader.__len__()):\n",
    "            best_model = model\n",
    "            best_accuracy = validation_accuracy/validation_loader.__len__()\n",
    "            best_epoch = epoch + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "O3OKTvWJ9g5c",
    "outputId": "3e13f043-4f66-4a6b-e2ef-d6ffb453150f"
   },
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "print(train_accuracy_all)\n",
    "print(validation_accuracy_all)\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "# ax1=fig.add_subplot(121)\n",
    "# ax1.plot(np.arange(1,11),train_accuracy_all)\n",
    "\n",
    "# ax2=fig.add_subplot(222)\n",
    "# ax2.plot(np.arange(1,11),validation_accuracy_all)\n",
    "\n",
    "plt.plot(np.arange(1,epoch_num+1),train_accuracy_all,'r',label='train accuracy')\n",
    "plt.plot(np.arange(1,epoch_num+1),validation_accuracy_all,'y',label='validation accuracy')\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xticks(np.arange(1,epoch_num+1))\n",
    "plt.title('The train accuracy and validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsVbvZFjQvLK",
    "outputId": "4ad1c577-c536-4a8a-8c46-212853d4a869"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "# PATH = './saved_model/model-10000-10-resnet18.pkl'\n",
    "# torch.save(best_model, PATH)\n",
    "\n",
    "# trained_model = torch.load(PATH)\n",
    "\n",
    "# save the parameters\n",
    "\n",
    "# PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-10000-10-resnet18.pkl'\n",
    "# torch.save(best_model.state_dict(), PATH)\n",
    "\n",
    "PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-mobilenetv3.pkl'\n",
    "torch.save(best_model.state_dict(), PATH)\n",
    "print(\"Accuracy for best model: \", best_accuracy/batch_size)\n",
    "print(\"Epoch: \", best_epoch)\n",
    "\n",
    "# trained_model = model()\n",
    "# trained_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45oDcCuy4MiG",
    "outputId": "c9c2fcba-16bd-46ed-d2d4-c3492e98caec"
   },
   "outputs": [],
   "source": [
    "# PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-resnet18.pkl'\n",
    "# trained_model = model()\n",
    "# trained_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18()\n",
    "in_feature_num = model.fc.in_features\n",
    "model.fc = nn.Linear(in_feature_num, 99)\n",
    "# model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3), padding=(3,3), stride=(2,2), bias=False)\n",
    "model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(5, 5), padding=(3, 3), stride=(2, 2),\n",
    "                            bias=False)\n",
    "# PATH = \"C:/ANU/2022 s2/honours project/code/UIED-master/model/model-99-resnet18.pkl\"\n",
    "PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-resnet18.pkl'\n",
    "# trained_model = model()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "validation_accuracy = 0\n",
    "validation_loss = 0\n",
    "\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(validation_loader):\n",
    "\n",
    "      validation_output = model(data[0].to(device))\n",
    "\n",
    "      data[1] = data[1].view(data[1].shape[0])\n",
    "\n",
    "      batch_loss = loss(validation_output, data[1].to(device))\n",
    "\n",
    "      validation_accuracy = validation_accuracy + np.sum(np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy())\n",
    "\n",
    "      if i == 0:\n",
    "        print(\"validation_output for i = 0: \", validation_output.cpu().data.numpy())\n",
    "        # print(\"index 47's' value in output: \", validation_output.cpu().data.numpy()[0][47])\n",
    "        print(\"max value of output for i = 0: \", np.max(validation_output.cpu().data.numpy(),axis=1))\n",
    "        print(\"label for i == 0: \", np.argmax(validation_output.cpu().data.numpy(),axis=1))\n",
    "        print(\"ground truth label: \", data[1].cpu().numpy())\n",
    "\n",
    "      # print('correct val in this batch: ', np.sum(np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy()))\n",
    "      # print('all correct val: ', validation_accuracy)\n",
    "      validation_loss = validation_loss + batch_loss.item()\n",
    "\n",
    "      # print(\"validation | num of right labelling: \", np.sum(np.argmax(validation_output.data.numpy(),axis=1) == data[1].numpy()))\n",
    "\n",
    "      c = (np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy())\n",
    "      c = c.squeeze()\n",
    "      for i in range(batch_size):\n",
    "        i_label = data[1][i]\n",
    "        class_correct[i_label] += c[i]\n",
    "        class_total[i_label] += 1\n",
    "\n",
    "print(\"validation accuracy: \", validation_accuracy/(validation_loader.__len__()*batch_size))\n",
    "print(\"validation loss: \", validation_loss/(validation_loader.__len__()*batch_size))\n",
    "\n",
    "for i, data in enumerate(validation_loader):\n",
    "  if i == 0:\n",
    "    print(data[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "FiLNInMPQvLK",
    "outputId": "024ff0c7-9a21-435f-f90f-aa631d958ff7"
   },
   "outputs": [],
   "source": [
    "# convert .npy file to image\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_data = np.load('/content/gdrive/MyDrive/mobile-semantics-classification/validation_x.npy')\n",
    "# print(sample_data[0,:,:,:].shape)\n",
    "array = np.reshape(sample_data[10,:,:,:], [32, 32])\n",
    "\n",
    "# array = np.array([array[1], array[0]])\n",
    "\n",
    "sample_image = Image.fromarray(array)\n",
    "# sample_image.show()\n",
    "plt.imshow(sample_image)\n",
    "# print(\"the array of image: \", np.asarray(sample_image))\n",
    "\n",
    "array = np.asarray(sample_image)\n",
    "array = array.astype('float32')\n",
    "array = array / 255\n",
    "array = (array - array.mean()) / array.std()\n",
    "\n",
    "print(\"array mean: \", array.mean())\n",
    "print(\"array std: \", array.std())\n",
    "\n",
    "array = array.reshape(1, 1, 32, 32)\n",
    "\n",
    "array = torch.tensor(array)\n",
    "array = array.permute(0,1,3,2)\n",
    "print(\"array_tensor: \", array)\n",
    "\n",
    "array_pred_label = model(array.to(device))\n",
    "print(\"output: \", array_pred_label)\n",
    "print(\"max value in output: \", np.max(array_pred_label.cpu().data.numpy()))\n",
    "print(\"index of max value in output: \", np.argmax(array_pred_label.cpu().data.numpy(),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "bRB0DHXp63j8",
    "outputId": "f77291aa-20c8-4fca-c9a8-d5409092cc9c"
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(validation_loader):\n",
    "  if i == 0:\n",
    "    # print(\"The first image of validation set: \", data[0][0])\n",
    "    first_image = np.reshape(data[0][0], [32, 32])\n",
    "    sample_first_image = Image.fromarray(np.array(first_image*255))\n",
    "\n",
    "    plt.imshow(sample_first_image)\n",
    "    # print(\"The array of image: \", np.asarray(sample_first_image))\n",
    "    # print(\"The array of image: \", first_image)\n",
    "\n",
    "    print(\"The tensorof image: \", data[0][0])\n",
    "\n",
    "    first_image_output = model(data[0].to(device))\n",
    "    print(\"output: \", (first_image_output.cpu().data.numpy())[0])\n",
    "\n",
    "    print(\"max value in output: \", np.max(first_image_output.cpu().data.numpy()[0]))\n",
    "    print(\"index of max value in output: \", np.argmax(first_image_output.cpu().data.numpy()[0]))\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "R9ydqseaQvLM",
    "outputId": "5557b234-63b1-4418-e17e-3e9751097e36"
   },
   "outputs": [],
   "source": [
    "# test existing model\n",
    "\n",
    "model_small_cnn = torch.load('/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/small_cnn_weights_100_512.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0MzgdKqjzaW",
    "outputId": "dd56887b-606d-4ecc-b668-0768980ceef6"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18()\n",
    "in_feature_num = model.fc.in_features\n",
    "model.fc = nn.Linear(in_feature_num, 99)\n",
    "# model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3), padding=(3,3), stride=(2,2), bias=False)\n",
    "model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(5, 5), padding=(3, 3), stride=(2, 2),\n",
    "                            bias=False)\n",
    "# PATH = \"C:/ANU/2022 s2/honours project/code/UIED-master/model/model-99-resnet18.pkl\"\n",
    "PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-resnet18.pkl'\n",
    "# trained_model = model()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btlq3Ak3aC4i"
   },
   "outputs": [],
   "source": [
    "# efficiency\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18()\n",
    "in_feature_num = model.fc.in_features\n",
    "model.fc = nn.Linear(in_feature_num, 99)\n",
    "# model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3), padding=(3,3), stride=(2,2), bias=False)\n",
    "model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(5, 5), padding=(3, 3), stride=(2, 2),\n",
    "                            bias=False)\n",
    "# PATH = \"C:/ANU/2022 s2/honours project/code/UIED-master/model/model-99-resnet18.pkl\"\n",
    "PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-resnet18.pkl'\n",
    "# trained_model = model()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "validation_accuracy = 0\n",
    "validation_loss = 0\n",
    "\n",
    "class_correct = list(0. for i in range(num_classes))\n",
    "\n",
    "class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(validation_loader):\n",
    "\n",
    "      validation_output = model(data[0].to(device))\n",
    "\n",
    "      data[1] = data[1].view(data[1].shape[0])\n",
    "\n",
    "      batch_loss = loss(validation_output, data[1].to(device))\n",
    "\n",
    "      validation_accuracy = validation_accuracy + np.sum(np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy())\n",
    "\n",
    "      if i == 0:\n",
    "        print(\"validation_output for i = 0: \", validation_output.cpu().data.numpy())\n",
    "        # print(\"index 47's' value in output: \", validation_output.cpu().data.numpy()[0][47])\n",
    "        print(\"max value of output for i = 0: \", np.max(validation_output.cpu().data.numpy(),axis=1))\n",
    "        print(\"label for i == 0: \", np.argmax(validation_output.cpu().data.numpy(),axis=1))\n",
    "        print(\"ground truth label: \", data[1].cpu().numpy())\n",
    "\n",
    "      # print('correct val in this batch: ', np.sum(np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy()))\n",
    "      # print('all correct val: ', validation_accuracy)\n",
    "      validation_loss = validation_loss + batch_loss.item()\n",
    "\n",
    "      # print(\"validation | num of right labelling: \", np.sum(np.argmax(validation_output.data.numpy(),axis=1) == data[1].numpy()))\n",
    "\n",
    "      c = (np.argmax(validation_output.cpu().data.numpy(),axis=1) == data[1].cpu().numpy())\n",
    "      c = c.squeeze()\n",
    "      for i in range(batch_size):\n",
    "        i_label = data[1][i]\n",
    "        class_correct[i_label] += c[i]\n",
    "        class_total[i_label] += 1\n",
    "\n",
    "print(\"validation accuracy: \", validation_accuracy/(validation_loader.__len__()*batch_size))\n",
    "print(\"validation loss: \", validation_loss/(validation_loader.__len__()*batch_size))\n",
    "\n",
    "for i, data in enumerate(validation_loader):\n",
    "  if i == 0:\n",
    "    print(data[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhwI6NMPbZ9R",
    "outputId": "3f4a5e86-e643-4898-b704-5c7b38e221eb"
   },
   "outputs": [],
   "source": [
    "# efficiency\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "\n",
    "# ------ resnet18 ----------------\n",
    "\n",
    "# model = models.resnet18()\n",
    "# in_feature_num = model.fc.in_features\n",
    "# model.fc = nn.Linear(in_feature_num, 99)\n",
    "# # model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3,3), padding=(3,3), stride=(2,2), bias=False)\n",
    "# model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(5, 5), padding=(3, 3), stride=(2, 2),\n",
    "#                             bias=False)\n",
    "# # PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-resnet18-epoch30.pkl'\n",
    "# PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-resnet18.pkl'\n",
    "# # trained_model = model()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# ------- semantic model --------\n",
    "\n",
    "# model = Net().to(device)\n",
    "# PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-semantic.pkl'\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# ------- mobilenetv2 ------------\n",
    "# model = models.mobilenet_v2().to(device)\n",
    "\n",
    "# model.features[0][0] = torch.nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "# num_features = model.classifier[1].in_features\n",
    "# model.classifier[1] = torch.nn.Linear(num_features, 99)\n",
    "\n",
    "# PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-mobilenetv2.pkl'\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# ------- mobilenetv3 ----------\n",
    "\n",
    "model = models.mobilenet_v3_small().to(device)\n",
    "\n",
    "model.features[0][0] = torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "model.classifier[-1] = nn.Linear(1024, 99)\n",
    "\n",
    "PATH = '/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-mobilenetv3.pkl'\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# ------- end --------------------\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Make predictions on the test DataLoader\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for inputs, targets in validation_loader:\n",
    "    inputs = inputs.to(device)  # Send data to GPU if available\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Get predicted labels\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Store true and predicted labels\n",
    "    y_true.extend(targets.cpu().numpy())\n",
    "    y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"processing time: %2.5f s\" % (time.time() - start_time))\n",
    "\n",
    "# # Make predictions on the test dataset\n",
    "# print(\"len(validation_loader[0]): \", len(validation_loader[0]))\n",
    "# validation_output = model.predict(validation_loader[0].to(device))\n",
    "# y_pred = np.argmax(validation_output.cpu().data.numpy(),axis=1)\n",
    "# print(\"y_pred: \", y_pred)\n",
    "\n",
    "# # Calculate the confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# # Calculate accuracy, precision, and recall\n",
    "# accuracy = accuracy_score(y_true, y_pred)\n",
    "# precision = precision_score(y_true, y_pred, average='macro')  # You can set the average parameter as needed ('micro', 'macro', etc.)\n",
    "# recall = recall_score(y_true, y_pred, average='macro')  # You can set the average parameter as needed ('micro', 'macro', etc.)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "  y_true[i] = y_true[i][0]\n",
    "\n",
    "labels_order = [72.0, 42.0, 77.0, 91.0, 6.0, 89.0, 40.0, 43.0, 82.0, 3.0, 68.0, 49.0, 56.0, 51.0]\n",
    "\n",
    "# Create a crosstab (contingency table) from y_true and y_pred\n",
    "print(\"y_true: \", y_true)\n",
    "print(\"y_pred: \", y_pred)\n",
    "contingency_table = pd.crosstab(pd.Series(y_true, name='True'), pd.Series(y_pred, name='Predicted'))\n",
    "\n",
    "# Print per-class accuracy for the interested classes\n",
    "for class_name in labels_order:\n",
    "    if class_name in contingency_table.index and class_name in contingency_table.columns:\n",
    "        true_positives = contingency_table.loc[class_name, class_name]\n",
    "        row_sum = contingency_table.loc[class_name, :].sum()\n",
    "        col_sum = contingency_table.loc[:, class_name].sum()\n",
    "        per_class_accuracy = true_positives / (row_sum + col_sum - true_positives)\n",
    "        print(f\"Accuracy for class {class_name}: {per_class_accuracy:.3f}\")\n",
    "\n",
    "# Alternatively, you can use the classification_report function to get a more comprehensive report\n",
    "# print(classification_report(gt_labels, pred_labels))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "average_precision = precision_score(y_true, y_pred, average='macro')\n",
    "average_recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average=None, labels=labels_order)\n",
    "\n",
    "recall = recall_score(y_true, y_pred, average=None, labels=labels_order)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=None, labels=labels_order)\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"average_precision: \", average_precision)\n",
    "print(\"average_recall: \", average_recall)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EP5IahvaubHQ"
   },
   "outputs": [],
   "source": [
    "# save entire model\n",
    "torch.save(model, \"/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-resnet18-entire.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puCKZv2GFVvs",
    "outputId": "af5a9c32-b80c-41e8-d378-31aadd35600b"
   },
   "outputs": [],
   "source": [
    "# need to use transformers for ViT\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVHVmBzeBya0",
    "outputId": "94032dae-69f9-4af2-99de-77cb9089e0a6"
   },
   "outputs": [],
   "source": [
    "# try ViT on rico\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# # Transformation to resize and normalize images\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # Load your own datasets\n",
    "# train_data = ImageFolder(root='path/to/train/data', transform=transform)\n",
    "# test_data = ImageFolder(root='path/to/test/data', transform=transform)\n",
    "\n",
    "# # Create data loaders\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "\n",
    "print(\"mean: \", mean)\n",
    "print(\"std: \", std)\n",
    "\n",
    "transform_train_vit = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    # transforms.Normalize(mean=mean, std=std), # featurewise centering\n",
    "])\n",
    "\n",
    "transform_val_vit = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "    # transforms.Normalize(mean=mean, std=std), # featurewise centering\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image = self.images[idx]\n",
    "\n",
    "        image = np.squeeze(image)\n",
    "\n",
    "        # image = ((image - image.min()) * (1/(image.max() - image.min()) * 255)).astype('uint8')\n",
    "\n",
    "        # image = Image.fromarray(image.astype('uint8'))\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform is not None:\n",
    "\n",
    "            image = self.transform(image)\n",
    "            # image = image.permute(1,0,2)\n",
    "\n",
    "\n",
    "        return image, label\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader_vit = DataLoader(CustomDataset(x_train[:113840],y_train[:113840],transform_train_vit), batch_size=batch_size, shuffle=True)\n",
    "# validation_loader = DataLoader(CustomDataset(x_train[9600:11200],y_train[9600:11200]), batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(CustomDataset(x_test[:1600],y_test[:1600]), batch_size=batch_size, shuffle=True)\n",
    "validation_loader_vit = DataLoader(CustomDataset(x_test[0:4800],y_test[0:4800],transform_val_vit), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "877cf46d0a874b9ab5e0f5d79cdba0e3",
      "3ee005df19c946f7987738b0ab89a335",
      "d1517a77aaa647fd8db71ec2a19a25fd",
      "0402938082b042a1af1e44edec6c94c2",
      "8f1e1194e09f49a79de223c11294e478",
      "11a8ae22ee8d452baadc98939db6f039",
      "812196d0009a45c8a1c8dac547d2ec29",
      "1cabaa26fa9444b68390027d1c4eb2f7",
      "7289903d1a6b4a6da76066999f33d994",
      "b58cd845012645c2960189c546950f4b",
      "d73a5e42bdc840fbb63715b07bd6e249",
      "41e8e6ce757147288b8cbac026485d9b",
      "8073497abfce46e69e856fa09528f29e",
      "ee2a7af98ee04a3b8de12f33d573ada2",
      "994252d526ef464aa7e045bc8a11879a",
      "66335dbb41684133b85b3a1aef7220ce",
      "a3e946a52369460db807abdf4052fc57",
      "37872d1c4b1240cba5bcdb12ba45181d",
      "611552f120f241009db76dc2d093c8a0",
      "7c78cc9bbfa44c46b8ef5882e31ed64d",
      "14dce6a7356244bbb068d73a65d4f4a8",
      "34940e8fe55147548fc1f6ce9895378a",
      "b5286efce04c495eb519c00a88a96f5a",
      "3e0ec5d6306f46f0b472ca53bc6bf831",
      "06f4134879d6462e808989afcf3f58a5",
      "eaa8410347ad4833969b16cedba068b7",
      "92175c2d5c5b45bfb37735384a9c84a9",
      "77e5dfadd347426f9e3bed2863e1fbc6",
      "4f8a8d409c9f4ce29114a71639c28e78",
      "a7ed48288c834c5ebd42e1f044e89376",
      "bdb0cdb562ae4a39b57e4d2975093405",
      "a3345076520b45b3b9437fe230c5f7a5",
      "cf58b3e9edab4e05ac1cd5a06b83eb84"
     ]
    },
    "id": "R2FkeR1UD_yT",
    "outputId": "8b53c0f6-2b1e-4981-f367-04f8e96aade8"
   },
   "outputs": [],
   "source": [
    "# continue ViT on rico\n",
    "\n",
    "# Define the feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224', do_normalize=True, image_mean=[mean.item()], image_std=[std.item()])\n",
    "\n",
    "# Load the pretrained model\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Adapt the model for 99 classes\n",
    "model.classifier = nn.Linear(model.config.hidden_size, 99)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     correct_predictions = 0\n",
    "#     total_predictions = 0\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     for images, labels in train_loader_vit:\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Perform forward pass\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         features = feature_extractor(images)\n",
    "#         outputs = model(**features)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(outputs.logits, labels)\n",
    "\n",
    "#         # Perform backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Calculate the running loss and the accuracy for this epoch\n",
    "#         running_loss += loss.item()\n",
    "#         _, preds = torch.max(outputs.logits, 1)\n",
    "#         correct_predictions += torch.sum(preds == labels)\n",
    "#         total_predictions += labels.shape[0]\n",
    "\n",
    "#     train_accuracy = correct_predictions.double() / total_predictions\n",
    "#     train_loss = running_loss / len(train_loader_vit)\n",
    "\n",
    "#     # Evaluation on test (or validation) data\n",
    "#     model.eval()\n",
    "#     running_loss = 0.0\n",
    "#     correct_predictions = 0\n",
    "#     total_predictions = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in validation_loader_vit:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             features = feature_extractor(images)\n",
    "#             outputs = model(**features)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "\n",
    "#             # Calculate the running loss and the accuracy for this epoch\n",
    "#             running_loss += loss.item()\n",
    "#             _, preds = torch.max(outputs.logits, 1)\n",
    "#             correct_predictions += torch.sum(preds == labels)\n",
    "#             total_predictions += labels.shape[0]\n",
    "\n",
    "#     validation_accuracy = correct_predictions.double() / total_predictions\n",
    "#     validation_loss = running_loss / len(validation_loader_vit)\n",
    "\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     print(f'Epoch: {epoch+1}/{num_epochs}, ',\n",
    "#           f'Train accuracy: {train_accuracy}, Train loss: {train_loss}, ',\n",
    "#           f'Validation accuracy: {validation_accuracy}, Validation loss: {validation_loss}, ',\n",
    "#           f'Time: {end_time - start_time} sec')\n",
    "\n",
    "\n",
    "# Best validation accuracy\n",
    "best_val_acc = 0\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_val_correct = 0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for images, labels in train_loader_vit:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        labels = labels.view(labels.shape[0])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        # Perform backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total_train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader_vit:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            labels = labels.view(labels.shape[0])\n",
    "\n",
    "            val_loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total_val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader_vit)\n",
    "    avg_val_loss = total_val_loss / len(validation_loader_vit)\n",
    "    train_acc = total_train_correct / len(x_train)\n",
    "    val_acc = total_val_correct / len(x_test)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "      best_val_acc = val_acc\n",
    "      best_model = model\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, Time: {epoch_time:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbR7Ye1iYZXo"
   },
   "outputs": [],
   "source": [
    "# save the ViT model\n",
    "torch.save(best_model, \"/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-ViT-entire.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnDgjVnQZmdl"
   },
   "outputs": [],
   "source": [
    "# vit efficiency on validation set\n",
    "\n",
    "\n",
    "# # Load the pretrained model\n",
    "# model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# # Adapt the model for 99 classes\n",
    "# model.classifier = nn.Linear(model.config.hidden_size, 99)\n",
    "\n",
    "# # Loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models\n",
    "import time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# ------- end --------------------\n",
    "\n",
    "# Load the model\n",
    "model = torch.load('/content/gdrive/MyDrive/mobile-semantics-classification/saved_model/model-99-ViT-entire.pkl')\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Don't calculate gradients\n",
    "with torch.no_grad():\n",
    "    for images, labels in validation_loader_vit:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# # Convert to numpy arrays\n",
    "# all_preds = np.array(all_preds)\n",
    "# all_labels = np.array(all_labels)\n",
    "\n",
    "# # Calculate accuracy, precision, recall and F1 score\n",
    "# accuracy = accuracy_score(all_labels, all_preds)\n",
    "# precision = precision_score(all_labels, all_preds, average='macro')\n",
    "# recall = recall_score(all_labels, all_preds, average='macro')\n",
    "# f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "# print('Validation Accuracy: ', accuracy)\n",
    "# print('Precision: ', precision)\n",
    "# print('Recall: ', recall)\n",
    "# print('F1 Score: ', f1)\n",
    "# print('Time cost for evaluation: ', elapsed_time, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGqbTHS1A9_V",
    "outputId": "23df42c6-eb38-4ee2-b90c-31be0ae9149b"
   },
   "outputs": [],
   "source": [
    "#continue ViT evaluation (table4_b)\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "interested_accuracy = []\n",
    "\n",
    "# print(y_true)\n",
    "\n",
    "# for i in range(len(y_true)):\n",
    "#   y_true[i] = y_true[i][0]\n",
    "\n",
    "labels_order = [72.0, 42.0, 77.0, 91.0, 6.0, 89.0, 40.0, 43.0, 82.0, 3.0, 68.0, 49.0, 56.0, 61.0]\n",
    "\n",
    "# Create a crosstab (contingency table) from y_true and y_pred\n",
    "print(\"y_true: \", y_true)\n",
    "print(\"y_pred: \", y_pred)\n",
    "contingency_table = pd.crosstab(pd.Series(y_true, name='True'), pd.Series(y_pred, name='Predicted'))\n",
    "\n",
    "# Print per-class accuracy for the interested classes\n",
    "for class_name in labels_order:\n",
    "    if class_name in contingency_table.index and class_name in contingency_table.columns:\n",
    "        true_positives = contingency_table.loc[class_name, class_name]\n",
    "        row_sum = contingency_table.loc[class_name, :].sum()\n",
    "        col_sum = contingency_table.loc[:, class_name].sum()\n",
    "        per_class_accuracy = true_positives / (row_sum + col_sum - true_positives)\n",
    "        print(f\"Accuracy for class {class_name}: {per_class_accuracy:.3f}\")\n",
    "        interested_accuracy.append(per_class_accuracy)\n",
    "\n",
    "# Alternatively, you can use the classification_report function to get a more comprehensive report\n",
    "# print(classification_report(gt_labels, pred_labels))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "average_precision = precision_score(y_true, y_pred, average='macro')\n",
    "average_recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average=None, labels=labels_order)\n",
    "\n",
    "recall = recall_score(y_true, y_pred, average=None, labels=labels_order)\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average=None, labels=labels_order)\n",
    "\n",
    "print(\"accuracy: \", accuracy)\n",
    "print(\"average_precision: \", average_precision)\n",
    "print(\"average_recall: \", average_recall)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print('Time cost for evaluation: ', elapsed_time, 'seconds')\n",
    "\n",
    "interested_precision = np.array(precision).sum()/len(precision)\n",
    "interested_recall = np.array(recall).sum()/len(recall)\n",
    "\n",
    "print(\"interested accuracy: \", np.array(interested_accuracy).mean())\n",
    "print(\"interested precision: \", interested_precision)\n",
    "print(\"interested recall: \", interested_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1E5FOAlpRU0m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjATGqU2SEJZ"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0402938082b042a1af1e44edec6c94c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b58cd845012645c2960189c546950f4b",
      "placeholder": "",
      "style": "IPY_MODEL_d73a5e42bdc840fbb63715b07bd6e249",
      "value": " 160/160 [00:00&lt;00:00, 9.57kB/s]"
     }
    },
    "06f4134879d6462e808989afcf3f58a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7ed48288c834c5ebd42e1f044e89376",
      "max": 346351599,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bdb0cdb562ae4a39b57e4d2975093405",
      "value": 346351599
     }
    },
    "11a8ae22ee8d452baadc98939db6f039": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14dce6a7356244bbb068d73a65d4f4a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cabaa26fa9444b68390027d1c4eb2f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34940e8fe55147548fc1f6ce9895378a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37872d1c4b1240cba5bcdb12ba45181d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e0ec5d6306f46f0b472ca53bc6bf831": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77e5dfadd347426f9e3bed2863e1fbc6",
      "placeholder": "",
      "style": "IPY_MODEL_4f8a8d409c9f4ce29114a71639c28e78",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "3ee005df19c946f7987738b0ab89a335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11a8ae22ee8d452baadc98939db6f039",
      "placeholder": "",
      "style": "IPY_MODEL_812196d0009a45c8a1c8dac547d2ec29",
      "value": "Downloading ()rocessor_config.json: 100%"
     }
    },
    "41e8e6ce757147288b8cbac026485d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8073497abfce46e69e856fa09528f29e",
       "IPY_MODEL_ee2a7af98ee04a3b8de12f33d573ada2",
       "IPY_MODEL_994252d526ef464aa7e045bc8a11879a"
      ],
      "layout": "IPY_MODEL_66335dbb41684133b85b3a1aef7220ce"
     }
    },
    "4f8a8d409c9f4ce29114a71639c28e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "611552f120f241009db76dc2d093c8a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66335dbb41684133b85b3a1aef7220ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7289903d1a6b4a6da76066999f33d994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77e5dfadd347426f9e3bed2863e1fbc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c78cc9bbfa44c46b8ef5882e31ed64d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8073497abfce46e69e856fa09528f29e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3e946a52369460db807abdf4052fc57",
      "placeholder": "",
      "style": "IPY_MODEL_37872d1c4b1240cba5bcdb12ba45181d",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "812196d0009a45c8a1c8dac547d2ec29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "877cf46d0a874b9ab5e0f5d79cdba0e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ee005df19c946f7987738b0ab89a335",
       "IPY_MODEL_d1517a77aaa647fd8db71ec2a19a25fd",
       "IPY_MODEL_0402938082b042a1af1e44edec6c94c2"
      ],
      "layout": "IPY_MODEL_8f1e1194e09f49a79de223c11294e478"
     }
    },
    "8f1e1194e09f49a79de223c11294e478": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92175c2d5c5b45bfb37735384a9c84a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "994252d526ef464aa7e045bc8a11879a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14dce6a7356244bbb068d73a65d4f4a8",
      "placeholder": "",
      "style": "IPY_MODEL_34940e8fe55147548fc1f6ce9895378a",
      "value": " 69.7k/69.7k [00:00&lt;00:00, 848kB/s]"
     }
    },
    "a3345076520b45b3b9437fe230c5f7a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3e946a52369460db807abdf4052fc57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7ed48288c834c5ebd42e1f044e89376": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5286efce04c495eb519c00a88a96f5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e0ec5d6306f46f0b472ca53bc6bf831",
       "IPY_MODEL_06f4134879d6462e808989afcf3f58a5",
       "IPY_MODEL_eaa8410347ad4833969b16cedba068b7"
      ],
      "layout": "IPY_MODEL_92175c2d5c5b45bfb37735384a9c84a9"
     }
    },
    "b58cd845012645c2960189c546950f4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdb0cdb562ae4a39b57e4d2975093405": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf58b3e9edab4e05ac1cd5a06b83eb84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1517a77aaa647fd8db71ec2a19a25fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cabaa26fa9444b68390027d1c4eb2f7",
      "max": 160,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7289903d1a6b4a6da76066999f33d994",
      "value": 160
     }
    },
    "d73a5e42bdc840fbb63715b07bd6e249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eaa8410347ad4833969b16cedba068b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3345076520b45b3b9437fe230c5f7a5",
      "placeholder": "",
      "style": "IPY_MODEL_cf58b3e9edab4e05ac1cd5a06b83eb84",
      "value": " 346M/346M [00:05&lt;00:00, 44.3MB/s]"
     }
    },
    "ee2a7af98ee04a3b8de12f33d573ada2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_611552f120f241009db76dc2d093c8a0",
      "max": 69665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7c78cc9bbfa44c46b8ef5882e31ed64d",
      "value": 69665
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
